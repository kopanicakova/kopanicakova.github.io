---
layout: archive
title: ""
author_profile: true
collection: publications
permalink: /publication/DD_SPIN
---

## <span style="color:rgb(199, 21, 133)"> Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies.  </span>
<div style="text-align: justify"> We propose to enhance the training of physics-informed neural networks (PINNs). To this aim, we introduce nonlinear additive and multiplicative preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear preconditioners are constructed by utilizing the Schwarz domain-decomposition framework, where the parameters of the network are decomposed in a layer-wise manner. Through a series of numerical experiments, we demonstrate that both, additive and multiplicative, preconditioners significantly improve the convergence of the standard L- BFGS optimizer, while providing more accurate solutions of underlying partial differential equations. Moreover, the additive preconditioner is inherently parallel, thus giving rise to a novel approach to model parallelism.
</div><br />


**Citation:** A. Kopaničáková, H. Kothari, G. Karniadakis and R. Krause. Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. arXiv:2302.07049, 2023.  <br />
**Download:** <a href="https://arxiv.org/pdf/2302.07049.pdf" style="color:rgb(199, 21, 133,0.75);">Preprint.</a> <br />


